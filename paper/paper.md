---
title: 'Exploring Bioinformatics in the Wild: Insights from Real LLM Conversations'
title_short: 'Wildchat analysis'
tags:
  - Wildchat
  - Large Language Model
  - Comparative analysis
authors:
  - name: David Steinberg
    orcid:
    affiliation: 1
  - name: Hirokazu Chiba
    orcid:
    affiliation: 2
  - name: Wasin Poncheewin
    orcid: 0000-0003-2267-6872
    affiliation: 3
  - name: Susumu Goto
    orcid:
    affiliation: 2
affiliations:
  - name: University of California, Santa Cruz Genomics Institute, 1156 High St Santa Cruz, CA 95064
    index: 1
  - name: Database Center for Life Science (DBCLS), Japan
    index: 2
  - name: National Center for Genetic Engineering and Biotechnology (BIOTEC), National Science and Technology Development Agency (NSTDA), Khlong Luang 10120, Thailand
    index: 3
date: 30 August 2024
cito-bibliography: paper.bib
event: BH24JP
biohackathon_name: "BioHackathon Japan 2024"
biohackathon_url:   "https://2024.biohackathon.org/"
biohackathon_location: "Fukushima, Japan, 2024"
group: R4
# URL to project git repo --- should contain the actual paper.md:
git_url: https://github.com/biohackathon-japan/bh24-characterize-biology-use-of-llms
# This is the short authors description that is used at the
# bottom of the generated paper (typically the first two authors):
authors_short: Steinberg D. \emph{et al.}
---

# Abstract

# Introduction

The intersection of artificial intelligence and conversational data presents a compelling frontier for research, particularly when exploring specialized fields such as biology and health sciences. The WildChat dataset, with its vast collection of over one million user-chatbot interactions, provides an unparalleled resource for examining how advanced language models engage with a diverse array of topics. Capturing dialogues generated by models like GPT-3.5 and GPT-4, WildChat not only offers a glimpse into everyday conversational dynamics but also holds potential for specialized inquiries.

Understanding how AI handles bioinformatics and health-related conversations is crucial for advancing both AI technology and domain-specific applications. The ability of conversational models to process and respond to complex biological queries can significantly impact research, education, and practical applications in health sciences. By delving into WildChat's dataset, we aim to uncover how these models interpret and manage discussions centered around biology and health, highlighting their effectiveness and areas for improvement.

Our study is driven by the objective to identify and analyze bioinformatics-related interactions within the dataset. Through a focused filtering process and subsequent evaluation using advanced tools, we explore how well these conversations are represented and how different models perform in handling them. This investigation not only sheds light on the capabilities of current conversational AI systems but also provides insights into their potential role in supporting and advancing biological and health science research.

# Related Work

The integration of conversational datasets into bioinformatics research is a burgeoning field with notable advancements and applications. One significant example is the SciQ dataset, a specialized resource that pairs scientific questions with answers, intended to enhance machine comprehension of scientific texts. SciQ exemplifies how conversational and question-answering datasets can be leveraged to improve the interpretability and utility of bioinformatics tools. By focusing on domain-specific queries, SciQ aids in training models to handle complex scientific language and context, which is crucial for bioinformatics applications.

In addition to SciQ, other research has explored the use of conversational datasets for improving scientific literature search and information retrieval. For instance, models trained on general conversational data have been adapted to support scientific inquiry by fine-tuning them on domain-specific corpora. This approach has shown promise in creating more effective tools for extracting and synthesizing information from large volumes of scientific texts.

However, these approaches face several limitations. Many models, while adept at general language understanding, struggle with the specialized vocabulary and context inherent in bioinformatics. Additionally, the existing datasets often lack the depth required for handling nuanced biological queries, which can limit the effectiveness of the models in real-world applications.

By addressing these limitations and expanding the scope of conversational datasets, researchers can develop more robust and context-aware bioinformatics tools. Exploring how conversational data can enhance model performance in handling biological questions offers a promising direction for future research and development in this field.

# Dataset and Filtering

![This figure outlines the analysis and software used. The WildChat dataset was filtered using hand-selected keywords. These prompts were then used to generate responses from a variety of other models. The prompts and responses were vectorized using a sentence embedding model, which were used in performing the analysis of the original and newly generated data. \label{Fig1}](./fig1.pdf)

The WildChat dataset, with its extensive collection of user-chatbot interactions, provides a rich resource for analyzing conversational patterns and exploring specialized domains such as bioinformatics. This dataset includes over a million conversations, capturing a diverse range of topics and user queries, making it a valuable tool for filtering and examining conversations relevant to biological and health sciences.

To identify conversations pertinent to bioinformatics, we employed a keyword-based filtering process. The initial phase involved selecting keywords believed to be relevant to biological research. These initial keywords included terms commonly associated with bioinformatics, such as "genome," "virus," and "bioinformatics." The intent was to capture a broad spectrum of conversations related to these topics.

However, as the filtering process progressed, we encountered challenges with certain broad keywords. For instance, terms like "family" and "kingdom," while relevant in a biological context, also have broader meanings that led to a significant amount of irrelevant data. This issue necessitated a refinement of our keyword list to enhance the specificity and relevance of the filtered conversations.

The refinement process involved iteratively adjusting our keyword selection based on the relevance of the filtered conversations. We employed additional techniques, including semantic analysis using sentence-transformers, to assess the alignment of the conversations with bioinformatics topics. This approach helped in filtering out noise and ensuring that the resulting dataset was more focused on the biological and health science domains.

Through these efforts, we aimed to create a curated subset of the WildChat dataset that better aligns with the objectives of our study, providing a more targeted foundation for subsequent analysis and model evaluation.

# Methods

## Language-Space Analysis

To gain insights into the conversational similarities within the filtered subset of the WildChat dataset, we utilized sentence-transformers. This technique involves embedding prompts and responses into a high-dimensional language space, allowing us to measure semantic similarities between them. By mapping the user prompts onto this space, we could analyze their relative positions and identify clusters of related topics within the bioinformatics domain. This analysis provided valuable information on how closely related different conversations are in terms of their content and language use, and helped us assess whether the prompts fell within the expected thematic areas of bioinformatics. It also revealed patterns and correlations that informed our understanding of the dataset's focus.

## Querying DBCLS Model API

The next step in our methodology involved leveraging the experimental API developed by DBCLS to query various models with the filtered prompts. The API was developed in Python using Flask and exposed as six endpoints, including gpt, gemini, claude3, commandr, llama, and mistral that accept the POST method with model parameters and interact with specific models. The backend of gpt endpoint interacts with OpenAI services deployed on Microsoft Azure. The backend of gemini endpoint interact with Gemini on Google Cloud. The backend of other endpoints interacts with Amazon Bedrock. For all models, temperature is set to 0 and max_tokens are set to 2048. The server developed using Flask includes an HTML and JavaScript code which provide an overview of the results.

In this phase, we designed our queries to explore the capability of different models to handle bioinformatics-related conversations. The queries were crafted based on the refined set of keywords and prompts identified during the filtering process, aiming to cover a range of bioinformatics topics and evaluate how effectively different models could address these queries.

We selected several models for comparison, including both established and experimental ones. By analyzing the responses generated by these models, we assessed their performance in handling bioinformatics-related content and compared their effectiveness. This comparison provided insights into the strengths and limitations of each model in the context of biological and health science conversations.

# Results

## Filtered Dataset Analysis

![Language embeddings of prompts were used to create a t-SNE. Hand curation of topics showed grouping of languages and topics. We found that our topic of interest was mostly represented between math, physics/chemistry, and health categories. \label{Fig2}](./fig2.pdf)

We begin by summarizing the characteristics of the conversations that remained after applying our filtering criteria. This analysis provides an overview of the types of discussions that were successfully identified as bioinformatics-related, including the frequency and diversity of topics covered. A set of 14 keywords were used to filter the dataset. The keywords are bioinformatics, biology, microbiology, notype, genus, phylum, taxonomy, prokaryote, bacteria, fungi, fungal, virus, eukaryote, and gene. This results in a total of 571 prompts from xxx prompts in the dataset. The prompt length was then calculated with the removal of special characters and “stop_words” (https://rdrr.io/cran/tidytext/man/stop_words.html). The distribution of the length is shown in Fig3. The frequency of the words from the prompt are shown in Fig4.

![Histogram showing the length of the prompts calculated with the removal of special characters and “stop_words”. \label{Fig3}](./fig3.pdf)

![Wordcloud showing the frequency of words within the prompts. \label{Fig4}](./fig4.pdf)

The prompts were categorized into different keywords with the inclusion of a “multi” group where there are multiple matches with the keywords. The embedded prompts were filtered for unique embedding, which resulted in 137 prompts in total. The prompts were projected onto the 2D plane using t-SNE [Fig5]. Distance between the represented points were calculated using Euclidean distances [Fig6]. The dendrogram cut-off was heuristically selected at the height of 67 to subset the prompts into 7 clusters. The language-space analysis revealed insightful patterns in how the prompts are distributed and related within the bioinformatics context. By examining the semantic similarities among the prompts, we identified clusters of related topics and assessed the coherence of the filtered dataset with our bioinformatics objectives. This analysis also highlighted the effectiveness of our filtering approach and revealed any potential gaps in the data.

![t-SNE plot of the projected unique embedded prompts. Different colors represent different keyword groups. \label{Fig5}](./fig5.pdf)

![caption](./fig6.pdf)\

![my Distances between different embedded prompts represented with a heatmap (top) and a dendrogram (bottom). Both x and y axes of the heatmap were labeled with the keywords using different colors. The cut-off for the dendrogram was heuristically selected at 67 to subset the prompts into 7 clusters, 1 to 7 from left to right”. \label{Fig6}](./fig6_2.pdf)

## Model Comparison

Our evaluation of the model performance involved analyzing how well it handled the bioinformatics queries compared to the baseline model in the WildChat dataset. This section details the strengths and weaknesses in generating relevant and accurate responses.

We also compared the DBCLS model with other models, focusing on their performance in biological reasoning and the quality of their responses to bioinformatics-related queries. This comparison provided a benchmark for understanding how well different models address the biological and health science topics. 

| Short Identifier | Full Model Name | Description | reference |
| ------------ | ----------- | --------------- | ------------- |
| gpt-35-turbo | GPT-3.5 Turbo | A variant of GPT-3.5 designed for improved speed and cost-efficiency in generating text. | https://papers.nips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf |
| gpt-4o | GPT-4 Optimized | An optimized version of GPT-4, offering enhanced performance for various language tasks. | OpenAI. Hello gpt-4o. https://openai.com/index/hello-gpt-4o/, 2024 |
| gpt-4 | GPT-4 | The fourth-generation GPT model, known for its advanced understanding and generation abilities. | https://arxiv.org/pdf/2303.08774 |
| claude3opus | Claude 3 Opus | A variant of Claude 3 designed for comprehensive and general-purpose conversational tasks. | https://arxiv.org/pdf/2404.13813, https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf |
| claude3sonnet5 | Claude Sonnet 3.5 | An enhanced version Anthropic Claude 3 Sonnet with improvements in text generation and comprehension. | https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf |
| claude3sonnet | Claude 3 Sonnet | A Claude model variant fine-tuned for creative text generation, such as writing poems or stories. | https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf |
| claude3haiku | Claude 3 Haiku | A Claude model variant specialized for speed. | https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf | 
| commandrplus | Command R Plus | An advanced model designed for natural language command execution and programming tasks. |  https://docs.cohere.com/docs/command-r-plus |
| commandrbasic | Command R Basic | A simpler version of the Command R series, suitable for executing straightforward commands. | https://docs.cohere.com/docs/command-r |
| llamallama3-70b | Llama 3 - 70 Billion Parameters | A large-scale model from the Llama series with 70 billion parameters, designed for various NLP tasks. | https://ai.meta.com/blog/meta-llama-3/, https://ollama.com/library/llama3:70b |
| llamallama3-8b | Llama 3 - 8 Billion Parameters | A smaller model in the Llama 3 series with 8 billion parameters, optimized for efficiency. | https://ai.meta.com/blog/meta-llama-3/, https://ollama.com/library/llama3:8b |
| llamallama2-70b | Llama 2 - 70 Billion Parameters | An earlier model in the Llama series, featuring 70 billion parameters for a range of tasks. | https://arxiv.org/pdf/2307.09288 |
| llamallama2-13b | Llama 2 - 13 Billion Parameters | A mid-sized model from the Llama 2 series with 13 billion parameters, balancing power and cost. | https://arxiv.org/pdf/2307.09288 |
| mistralmistral-7b | Mistral - 7 Billion Parameters | A compact model from the Mistral series with 7 billion parameters, focused on effective NLP. | https://arxiv.org/pdf/2310.06825, https://docs.mistral.ai/getting-started/models/ |
| mistralmixtral-8x7b | Mixtral - 8x7 Billion Parameters | An ensemble model combining eight 7-billion parameter models from the Mistral series. | https://arxiv.org/pdf/2401.04088, https://docs.mistral.ai/getting-started/models/ |
| mistralmistral-large | Mistral Large | A more extensive model in the Mistral series with enhanced capabilities for diverse applications. | https://docs.mistral.ai/getting-started/models/ |

The evaluation included an assessment of each model's capabilities in reasoning about biological concepts, handling complex queries, and generating informative responses. This comparison helps to contextualize the DBCLS model's performance within the broader landscape of conversational AI in bioinformatics. We generated embeddings for each response from the filtered dataset using Mixed Bread AI Large v1 via the Hugging Face sentence transformers library. We then used these embeddings to generate distance calculations to the original response from WildChat. In addition, Levenshtein distance was used to compare responses from models to the original response.

![This box plot shows the distribution of Levenshtein distances from the original response provided by ChatGPT-4 as part of the WildChat dataset. Notably, the Llama3-8b and Mistral AI 7b models showed the greatest divergence from the baseline model. In addition, it appears that the GPT-3.5 Turbo model generated responses that conformed well to the baseline.”. \label{Fig7}](./fig7.pdf)

## Characterizing Model Responses

In this section we will look at a few handpicked prompts and responses to characterize some of the bioinformatics related use cases that were observed in the WildChat dataset. 

### Translation Tasks

First, it appears that a user intended to translate a caption from a paper from Japanese to English.

Pull out a few question and answer pairs

```
Prompt: \\翻译为英文“A图为上调基因的GO,KEGG分析可视化的气泡图；B图为下调基因的GO,KEGG分析可视化的气泡图；C图为上调基因的富集术语的弦图；D图为下调基因的富集术语的弦图”
Response: "The figure A shows the bubble chart for the upregulated genes in GO and KEGG analysis. The figure B shows the bubble chart for the downregulated genes in GO and KEGG analysis. The figure C shows the chord diagram for the enriched terms of upregulated genes. The figure D shows the chord diagram for the enriched terms of downregulated genes."
```

### Investigating Lab Procedures

```
Prompt: A rapid and cost-effective method for genotyping apolipoprotein E gene polymorphism
```

Successful responses followed the instruction by providing the user with a method that mentioned things like DNA extraction, PCR in a series of steps. ChatGPT, CommandR, and Claude all created reasonable sounding responses that would help in genotyping APOE at times including details of reagent preparation. An extract of an exemplary response provided by Chat GPT-4o is provided below.

```
Model gpt-4o Response: 
2. **PCR Amplification**:

  - Design primers flanking the polymorphic region of the APOE gene.
  - Set up the PCR reaction mix:
  - 1x PCR buffer
  - 200 µM dNTPs
  - 0.2 µM of each primer
  - 1.5 mM MgCl2
  - 1 U Taq DNA polymerase
  - 50-100 ng of genomic DNA
  - PCR cycling conditions might be:
  - Initial denaturation at 95°C for 5 minutes
  - 35 cycles of:
  - Denaturation at 95°C for 30 seconds
  - Annealing at 58-60°C for 30 seconds
  - Extension at 72°C for 1 minute
  - Final extension at 72°C for 5 minutes
```

It is also worth noting the accuracy of the Claude 3 Opus model in correctly retrieving the RefSNP ID of two variants on the APOE gene when responding to the genotyping query. An extract of the response is provided below (our emphasis):

```
Model: claude3opus Response:
  
2. PCR amplification:

 - Design primers specific to the APOE gene region containing the polymorphisms of interest (e.g., rs429358 and rs7412).
```

Note that both rs429358 and rs7412 are indeed located in the APOE gene.
